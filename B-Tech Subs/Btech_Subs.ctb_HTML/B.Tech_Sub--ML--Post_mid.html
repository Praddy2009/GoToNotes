<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Post mid</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles3.css" type="text/css" />
  
    <script type="text/javascript">
        function in_frame () { try { return window.self !== window.top; } catch (e) { return true; } }
        if (!in_frame()) {
            var page = location.pathname.substring(location.pathname.lastIndexOf("/") + 1);
            window.location = 'index.html#' + page;
        }
    </script>
</head>
<body><div class="page"><h1 class="title">Post mid</h1><br/><strong>Concept of Kernels and Basis Expansion</strong><br /><br />Let's say you wanna buy a mobile phone, On research you found two phones with exact same configurations you looking for.<br />            <img src="images/44-1.png" alt="images/44-1.png" /><br />            <br />Now the confusion is which one to choose. So, you asked your friend. He told that in city we have a service center of Apple I5 but not for Zobio 12. i.e he added a basis for decision.<br /><br />              <img src="images/44-2.png" alt="images/44-2.png" /><br />              <br />If we try to understand it geometrically then,<br /><img src="images/44-3.png" alt="images/44-3.png" /><br /><br />In <strong>Geometry →  Axis</strong><br /><strong>    Linear Algebra →  Bias</strong><br /><br /><strong>Que</strong> : What will be the effect on data after introducing the basis?<br /><strong>Ans</strong> : Adding a dimension help us in linearly seperate the data<br /><img src="images/44-4.png" alt="images/44-4.png" /><br /><br /><strong>Que:</strong> What challenges we face on introducing basis expansion?<br /><strong>Ans</strong> : → We have to transform the points<br />          → We have to calculate the similarity w.r.t this hyperplane.<br />          <br /> These tasks are massive in terms of computation.<br /> <br /> <img src="images/44-5.png" alt="images/44-5.png" /><br /> <br />Now to transform 3D to 9D we have to do so much computation.<br /> <br /> What if I give you a function <strong>K()</strong> //also known as kernel function.<br />                <img src="images/44-6.png" alt="images/44-6.png" /><br />                <br />Dot product of X' &amp; Y' from above also 1024 and via kernel function also 1024 but here the computation is much less.<br /><br />           <img src="images/44-7.png" alt="images/44-7.png" /><br />           <br />           <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><br /><strong><span style="text-decoration:underline;">Support Vector Machine</span></strong><span style="text-decoration:underline;"> (SVM)</span><br /><br />Let's understand a scenerio first. We are given some points and we trying to fit a classifier to it.<br /><br />       <img src="images/44-8.png" alt="images/44-8.png" /><br />       <br />Now in <strong>least square method</strong><br />            → Every data point contributes towards the decision making which increases computation<br />            <br />         in <strong>KNN</strong><br />             → Highly dependent on K (NP Hard)<br />             → Need to compute for each data which leads to increased computation.<br />             <br />             <br /><strong>Sol</strong> : Now if we observe the distribution then we find that chances of missclassification of A1 and A2 are too low(least contribution in boundary position).<br /><br /><img src="images/44-9.png" alt="images/44-9.png" /><br /><br />So the <strong>Idea</strong> of SVM is for decision on position for boundary<br /><strong>   </strong><strong><span style="background-color:#ff0900;"> Consider only those terms whose chances are high to get affected by the position of the boundary.</span></strong><strong><br /></strong><br />Let's fine tune the same idea.<br /><br /><strong><span style="text-decoration:underline;">Maximum Margine Classifier</span></strong><br /><br />Let's say we are given a data distribututed like the one shown below and we need to decide the position of boundary and the boundary width.<br /><br /><img src="images/44-10.png" alt="images/44-10.png" /><br />Now the question is who will decide where should the boundry lie so that no miss classification happens.<br /><br />The <strong>Idea</strong><br />    Consider only those data who has the highest probability of being miss classified. We take those element and draw the boundary at the mean point of those data.<br /><br /><strong>Que</strong> : What could be major problem / limitation for this classifier?<br /><strong>Ans</strong> : → Suppose we have an outlayer(blue point). So we need to shift our boundary i.e. Boundary is sensitive to the outliers(SOFT Boundary/ Soft Margin) and we can have more than one soft boundary(so now the problem arises to find the better out from it and it became <strong>optimization problem</strong> now).<br /> <br />  <img src="images/44-11.png" alt="images/44-11.png" /><br />            <img src="images/44-12.png" alt="images/44-12.png" /><br /> <strong>Que</strong> So what is SVM?<br /> <strong>Ans</strong> These are the points which are residing atleast on the decision edge or boundary of the decision edge and we define the boundary based on these points.<br /><br /><strong>Example</strong><br /><br />Let's say we are given the following distribution<br />                <img src="images/44-13.png" alt="images/44-13.png" /><br />                <br />So the blue circled imgs are choosen as the support vectors as they have the chance of getting missclassified. But now the question is how to draw the boundary.<br /> <br />                 <img src="images/44-14.png" alt="images/44-14.png" /><br />          <br />The margin can be choosen such that<br />        → The seperation between margin is maximum<br />        → A straight line can be drawn in middle of these margins.<br />           <img src="images/44-15.png" alt="images/44-15.png" /><br />           <br /><strong>Task</strong> : Can we imagine a straight line to seperate the data?<br />            <img src="images/44-16.png" alt="images/44-16.png" /><br /><strong>Sol</strong> : No we can't. This is liniary non seperable data.    <br />With the existing dimension we can't seperate so what we will do we'll add one more feature(basis expansion). But we know that basis expansion comes with various challanges..let's have a look on some examples<br /><br />Let's say we given,<br /><br />            <img src="images/44-17.png" alt="images/44-17.png" />  <br /><br />So as it is not liniarly seperable we introduce one more dimension to it transform the points governed by the transformation function f=x<sup>2</sup>.<br /><br />             <img src="images/44-18.png" alt="images/44-18.png" /><br />             <br />Now it's linearly seperable.             <br />             <br />            <img src="images/44-19.png" alt="images/44-19.png" /><br />Now a kernel function does all the 3 task in one go.<br /><br /><strong><span style="text-decoration:underline;">Polynomial Kernel</span></strong><strong> </strong><br />        <img src="images/44-20.png" alt="images/44-20.png" /><br />        <br />Example 1:<br />        <img src="images/44-21.png" alt="images/44-21.png" /><br />        <br />Example 2:<br />        <img src="images/44-22.png" alt="images/44-22.png" /><br /><br /><br />            <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Decision Tree</strong><br />It is a greedy hierachical decision making model which can be used for regression and classifier depending upon the target attribute.<br /><br />Steps involved:<br />   ⇒ Deciding the splitting attribute at each level<br />   ⇒ Deciding the pruning point. <br /><br />Whenever we try to solve a decision tree we get three questions(<span style="background-color:#ff0900;">  </span>):<br /><img src="images/44-23.png" alt="images/44-23.png" /><br /><br />Terminology of decision tree:<br /><br />             <img src="images/44-24.png" alt="images/44-24.png" /><br />             <br /><strong>Benefit of splitting</strong> : Step by step it reduces the data set.<br /><br /><strong>Pruning</strong> : The process of limiting tree to only a specified depth and avoid having Decision/terminal nodes beyond that. It helps to avoid <strong>overfitting</strong>.<br /><br />            <img src="images/44-25.png" alt="images/44-25.png" /><br />            <br />Now the 1st question , from where to start   <br /><br />   <img src="images/44-26.png" alt="images/44-26.png" /><br />            <br /><br />    <img src="images/44-27.png" alt="images/44-27.png" /><br />    <br />  <img src="images/44-28.png" alt="images/44-28.png" /><br />  <br />  <img src="images/44-29.png" alt="images/44-29.png" /><br />  <br />  <strong>Splitting Criteria</strong><br />  <br />  1) <strong>Ginne Index</strong><br />  <img src="images/44-30.png" alt="images/44-30.png" />  <br />  <img src="images/44-31.png" alt="images/44-31.png" /><br />  <br />  <strong><br /></strong><br />2) <strong>Chi Square</strong><br />               <br />           <img src="images/44-32.png" alt="images/44-32.png" /><br />           <br />  Steps to calculate the chi- square<br />  <br /> <img src="images/44-33.png" alt="images/44-33.png" /><br /> <br />Similarly we try finding chi square for splitting based on different factors and we'll choose the one with highest value.<br /><br /><span style="text-decoration:underline;">Properties of Chi- Square</span><br />                    <br />                    <img src="images/44-34.png" alt="images/44-34.png" /><br />                    <br /><br />3) <strong>Information Gain</strong><br />                 <br /><strong>                Information Gain = 1- Entropy</strong><br />                <br />     Entropy : randomness / Disorganisation/ Impurity<br />     <br />                      <img src="images/44-35.png" alt="images/44-35.png" />     <img src="images/44-36.png" alt="images/44-36.png" /><br />                      <br />Let's see a data<br />                    <img src="images/44-37.png" alt="images/44-37.png" /><br />So as we move to the left we'll need more information to understand the data as it's getting impure.<br /><br />    Steps to calculate entropy of the node (For gender class)<br /><br /><img src="images/44-38.png" alt="images/44-38.png" /><br /><br />So then we find information gain and the one which gives more info we choose that as splitting attr.<br /><br />4) <strong>Reduction in Variance</strong><br />   This is what was our main motive behind all the above 3 options. Previous methods are only aplicable for categaorial data but this method could be used by continuous data. <br />                  <img src="images/44-39.png" alt="images/44-39.png" /><br />                  <br /><br /><strong>Comparison of Tree based model with Linear Model<br /></strong>   ⇒ Choice of the model depends upon the data and type of problem.<br />   ⇒ Understand the relation between independent attribute and dependent attribute.<br /><br />                  <img src="images/44-40.png" alt="images/44-40.png" />     <br />                  <br />                 <br /><strong>Avoiding Overfitting in tree based models</strong>                 <br /><br />    <img src="images/44-41.png" alt="images/44-41.png" /><br />    <br />    We need to achieve a better bias variance tradeoff.<br />    <br />Let's look a <strong>Underfitted tree</strong><br /><br />Let's say we have 100 attributes but we chose only two attributes as feature.<br /><img src="images/44-42.png" alt="images/44-42.png" /><br /><br /><br />Let's look a <strong>Overfitted tree</strong><br />Again in data we have 100 attribute we used 98 attributes as features.<br /><br />   <img src="images/44-43.png" alt="images/44-43.png" /><br />   <br />Policies to avoid Overfitting<br />   ⇒ Setting up the constraints on tree size<br />      • Max feature : Max no of features can be used<br />      • Min sample split :<br />      • Minimum sample number/ Max depth :<br />   <br />   ⇒ Tree pruning/Tree cutting<br />      • We let the tree grow<br />      • Discard the branches that doesn't yield better result.<br />      <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Random Forest</strong><br />It's a data structure that is made up of more than one tree.<br /><br />Process:<br />   ⇒ Grow multiple tree simultaneously<br />   ⇒ Discard the tree that does not perform well.<br /><br />                ----------------------------------------------------------------------------------------------------------------------<br />                            <strong>Ensemble Method</strong><br />                              → Group of predictive models to achieve better accuracy and stability<br />                              →  Better Bias Variance tradeoff.<br />                              <br />                            Example: <br />                               <img src="images/44-44.png" alt="images/44-44.png" /><br />                               <br />                                <strong>BAGGING</strong> <strong>Stage</strong><br />                                    →It reduces the variance.   <br />                                        <img src="images/44-45.png" alt="images/44-45.png" /><br /><br />                 ------------------------------------------------------------------------------------------------------------<br />                 <br /> Random Forest:<br />   ⇒ Deploy more than one decision tree<br />   ⇒ Voting mechanism is used to discard unperforming trees<br />   ⇒ Increased Computation<br />   ⇒ Black Box Model<br />   ⇒ Can work as both classifier and regression but not good<br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><strong>Expressing  The Margin width in terms of Model Parameter</strong><br /><br />        <img src="images/44-46.png" alt="images/44-46.png" /><br />        <br /> <img src="images/44-47.png" alt="images/44-47.png" />   <img src="images/44-48.png" alt="images/44-48.png" /><br /> <img src="images/44-49.png" alt="images/44-49.png" /><br /> <img src="images/44-50.png" alt="images/44-50.png" /><br /> <img src="images/44-51.png" alt="images/44-51.png" /><br /> <br /> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br />PCA<br />→ It reduces dimension of data.<br /><br /><strong>Que</strong>: Why we reduce the dimensions?<br />Ans :<br />→It trims the data set. Model has to process less number of data.<br />→Helps in avoiding multicoliniarity. Avoids Overfitting.<br />→ Only the relavant info we process.<br /><br /><strong>Que</strong>: Can we pick any random column/attribute and drop it? <br /><strong>Ans</strong>: No<br /><br /><strong>Que</strong>: What should be the criteria?<br /><strong>Ans</strong>: Dropping that column doesn't affect the learning.<br /><br /><strong>Que</strong>: How should we reduce the dataset? <br /><strong>Ans</strong>: This is primerely done by two process:<br />   ⇒ Feature Extraction<br />      • Takes attribute as a feature<br />      • Takes Linear combination of some attributes to form a featutre.<br />                <img src="images/44-52.png" alt="images/44-52.png" /><br />                <br />                <br />PCA is a feature extraction mechanism.<br /><img src="images/44-53.png" alt="images/44-53.png" /><br /><br />                <br /><br /><br />            <br /><br /> <br />     <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></div></body></html>