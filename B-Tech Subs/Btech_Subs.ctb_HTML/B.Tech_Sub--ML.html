<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>ML</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles3.css" type="text/css" />
  
    <script type="text/javascript">
        function in_frame () { try { return window.self !== window.top; } catch (e) { return true; } }
        if (!in_frame()) {
            var page = location.pathname.substring(location.pathname.lastIndexOf("/") + 1);
            window.location = 'index.html#' + page;
        }
    </script>
</head>
<body><div class="page"><h1 class="title">ML</h1><br/><strong>Mr Jhalak Hota</strong><br /><br /><img src="images/4-1.png" alt="images/4-1.png" /><br /><br /><img src="images/4-2.png" alt="images/4-2.png" /> <br /><br /><img src="images/4-3.png" alt="images/4-3.png" />                                                        <img src="images/4-4.png" alt="images/4-4.png" /><br /><br /><img src="images/4-5.png" alt="images/4-5.png" />                                 <img src="images/4-6.png" alt="images/4-6.png" /><br /><br /><img src="images/4-7.png" alt="images/4-7.png" /><br /><br /><strong>Note</strong> :  Internally regression and classification are same<br /><br /><img src="images/4-8.png" alt="images/4-8.png" /><br /><br /><img src="images/4-9.png" alt="images/4-9.png" /><br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Model</strong> :  These are rules that segregates the instance(Solution) space<br /><br /><img src="images/4-10.png" alt="images/4-10.png" /><br /><br /><strong>Que</strong>: How the models are expressed ?<br />Sol: 1) <strong>Logical Rules</strong> - Logical modeling<br />            Ex: Model specifying surviving criterion for Titanic ship logic <br />            <img src="images/4-11.png" alt="images/4-11.png" /><br />            <br />       2) <strong>Geometric</strong> - Geometric Modeling<br />            Ex - <br />            <img src="images/4-12.png" alt="images/4-12.png" /><br />            <br />            <br />       3) <strong>Probability</strong> - Probabilistic Modeling<br />            Ex: Mostly used for classification problems<br />                  <strong>Baye's Theorem</strong><br />            <img src="images/4-13.png" alt="images/4-13.png" /><br />            <br /><br /><strong>Note</strong>: Let's say we have a scenerio where we just want to analyze the data and find the distances between the observations. So for this we need to use distance metrics.<br /><br />   <img src="images/4-14.png" alt="images/4-14.png" /><br />   Using different distance metrices like euclidian, manhatten etc(each used in different scenerios) we found the various distances and come to know that two X which are highlighted with red are closer than the others. <br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Methods of learning</strong><br /><br /><strong>    1) Least Square Method (Linear Model)</strong><br />     <strong>2)</strong> <strong>K</strong>- <strong>Nearest Method</strong> <strong>(Classification)</strong><br /><strong>    3) Distance Based Model(Unsupervised Learning)</strong><br /><br />Que: What is learning?<br />Ans:  Changing the coefficients of the model to achieve a better accuracy in the ML task.<br /> <br />    Let's see a scenerio of Linear model, <br />    <img src="images/4-15.png" alt="images/4-15.png" /><br />    <br />    So as metioned we assumed f'(X) indicating that we try to relate/Map X -&gt; Y using a straight line. <br /><br />    But now there could be many lines like<br /><br />    <img src="images/4-16.png" alt="images/4-16.png" /><br />    Each line is identified by its intercpt and slope(as eq of line Y= MX + C ). So during learning we try to adjust the values of M(slope) and C(intercept) so that when we input X it gives correct Y and if it(model) gives for majority of our data then we got the correct model else again we adjust the values of M and C and try again. <br />    <br />  In successive classes we will get to know that the changes in intercept(a.k.a bias) is prohibited and mostly changes are done in slope.<br />    <br />    <br /><strong>Least Square Method</strong> (<strong>Linear Model</strong>)<br /><br />    <img src="images/4-17.png" alt="images/4-17.png" /><br />                    <img src="images/4-18.png" alt="images/4-18.png" /><br /><br />    <br />    The Bias (B0) :<br /><br />    <img src="images/4-19.png" alt="images/4-19.png" /><br />    <br />    <strong>Note</strong> : During machine learning small change is OK and we mostly change the slope(coefficient B1). If we find the significant change in B0 then the model is prone to overfitting.<br />    <br />    Let say the model is represented as <br />                    Y= B0 + B1X1 + B2X2<br />                    <br />     Then during learning values of B1 and B2 are adjusted and we try not to adjust B0.<br />     <br /> <br /> <strong>Que</strong>: What is learning Rate/ Rate of learning?<br /> <strong>Ans</strong>: Amount of change being performed on each iteration.<br /> Let's say at (i+1)th iteration<br /> <br />            <img src="images/4-20.png" alt="images/4-20.png" /><br />            <br /><strong>Que</strong> What is the optimal rate of learning? What parameters affect that?<br /><strong>Ans</strong> : Will be given after understanding least square method.<br /><br /><strong>Least Square method</strong><br /><br />        <img src="images/4-21.png" alt="images/4-21.png" /><br />Least square method aims to minimize the total <strong>Sum of square error(E)</strong><br /><strong>Note</strong>: Only suitable for linear Models<br />        <br />                    <img src="images/4-22.png" alt="images/4-22.png" /><br /><strong>Que</strong> : Why we taking square on the above equation?<br /><strong>Ans</strong>  : We square in the above equation so that we get a convex surface(parabola) which<br /><br />        -&gt; Ensures minimizing the error<br />        -&gt; Guides to get the optimized rate of learning.<br />        <br />Let's plot the error vs B(model parameter)<br /><br />    <img src="images/4-23.png" alt="images/4-23.png" />'<br />    <br />Now when we developed a model we can lie/spawn at any point on the curve and then need to do the changes in the coefficients to reduce the error. Here comes the major importance of rate of learning. Let say in both the cases we landed at point R1 and now we take different rate of learning in both the cases.<br /><br />   <img src="images/4-24.png" alt="images/4-24.png" /><br />   <br />            High learning Rate : We may miss the optimal parameter<br />            Low Learning Rate : The convergence is very slow<br />            <br />            So, we need to find the optimal rate of learning.    <br /><br /><strong>Limitation of Least Square method</strong><br />    -&gt; Works perfectly only with linear models<br />    -&gt; Linear models has it's shortcomings, so as least square <br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Distance Based Model</strong> (Unsupervised Learning)<br /><br />â†’ Mostly used in unsupervised learning.<br /><br />    <img src="images/4-25.png" alt="images/4-25.png" /><br />    <br />    Let's say we need to do a unsupervised learning(Y is not known). So we try finding the distances between various points to conclude something.<br />        <br />          dis(P1P2) &lt;  dis(P1P3)  &lt; dis(P1P4)<br />          <br />          So P2 is closer to P1 than P3.<br />          <br />   This is called Distance based model.<br />   <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Limitation of liner Model</strong><br /> -&gt; <strong>Linear Model don't fit properly for classification</strong><br />      Ex: Let's say in the below graph we want a model to classify blue and Yellow.<br />      <img src="images/4-26.png" alt="images/4-26.png" /><br />      In this case we found that Linear model is not appropriate to use/implement.<br />      <br />      Now for the case above we need more zig-zag kind of line(model). For the same nearest neighbour model was developed.<br /><br /><br /><strong>Nearest Neighbour Method (Classification)</strong><br /><br />Let's say we are given following scenerio<br /><br /><img src="images/4-27.png" alt="images/4-27.png" /><br /><br />Now to know which class the <strong><span style="background-color:#3fe90f;">X</span></strong> belongs we need some way to decide that.<br /><br />Here we'll use/see the neighbours of <strong><span style="background-color:#3fe90f;">X</span></strong> to determine which class it should belong to. In order to do that we'll find distance between <strong><span style="background-color:#3fe90f;">X</span></strong> and it's neighbours.<br /><br /><strong>Que</strong> : We'll calculate the distance but with how many neighbours ? <br /><br /><img src="images/4-28.png" alt="images/4-28.png" /><br /><br />As you can see that with increased number of neighbours computation will be more.<br /><br />In mathematical terms we write,<br />        <img src="images/4-29.png" alt="images/4-29.png" /><br /><br /><strong>Que</strong> : So the question remains the same how many neighbours to consider?<br /><strong>Ans</strong> : This kinds of problem is NP- complete problems i.e. non deterministically polynomial hence we can't reach to any particular decision. So we have to go for trial and error(Brute force).<br /><br />Let's see how our model is affected if we take different number of neighbours.<br /><br /><img src="images/4-30.png" alt="images/4-30.png" />                              <img src="images/4-31.png" alt="images/4-31.png" />        <br />     (Fig1)   15 Neighbours                                                              (Fig 2)  1 Neighbour<br />              <br /><strong>Que</strong> :  But which one should we choose?<br /><strong>Ans</strong> : Fig 1 must be used. Fig2  is rejected because it doesn't generalise(ability of model to capture the unknown) ,highly <strong>overfitted</strong>, high variance and low bias. <br /><br /><strong>Advantage of nearest neighbour model</strong><br />â†’ Only nearest neighbours are involved in defining the model instead of all other data(as we see in LM).<br /><br /><img src="images/4-32.png" alt="images/4-32.png" /><br /><br /><img src="images/4-33.png" alt="images/4-33.png" /><br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Overfitting </strong><br />    It's the condition where our model gives<br />        -&gt; Best result for seen data/ Train data <br />        -&gt; Worst Result for unseen data/ Test data<br />        <br />    For ex: Let's say your model is a student and he's preparing for exams. He mugged up the solutions and gave exam. Now exam can be of two categories.<br />    <br />    <strong>Case 1:</strong> Same question with same value came : So in this case our model passed with flying colour as all the questions have the same data and our model just needs to reproduce it and pass exam with high accuracy.<br />    <br />    <strong>Case 2:</strong> Same question with different data came and this time again our model just wrote the solution that he mugged up now this is where he wen't wrong and failed miserably.<br /><br />    <strong>Case Study</strong> : To understand overfitting let's see a case study.<br />    Let's say we have a set of observations as shown below.<br />    <img src="images/4-34.png" alt="images/4-34.png" /><br />    <br />    So we applied linear model to it,<br />    <img src="images/4-35.png" alt="images/4-35.png" /><br />    But we found that in region A most of the observations are too away from our model.So we thought when not bring some curve to our model such that it covers more points.<br />    <br />    <br /><img src="images/4-36.png" alt="images/4-36.png" /><br /><br />In this model we found that yes it covers more of the observation (i.e. better model with better results). Now we tempted to add more curves and came up with model shown below.<br /><br /><img src="images/4-37.png" alt="images/4-37.png" /><br /><br />Ok we got a really high accuracy model with better results but what about the complexity and computation power. We can see that the model is in terms of power of 6 so it will increase the computation and time and will be discarded for same. This is the proper example of overfitted model.(i.e. it has great accuracy but not optimal in terms of computation and some other parameters).<br /><br />So the first one is underfitted second is perfect fit and the last one is overfitted model.<br /><br />So one more example<br />    <img src="images/4-38.png" alt="images/4-38.png" /><br /><br /><br /> <img src="images/4-39.png" alt="images/4-39.png" /><br /> <br /> In overfitting model is : <strong>Low Bias and High Variance</strong><br />         <br /><br /><strong>Que:</strong>  So where to stop or which model to consider?<br /><strong>Ans:</strong>  <img src="images/4-40.png" alt="images/4-40.png" /><br /><br />We should stop where the total tran error is minimum and test error and train error are small.<br /><br /><strong>Note</strong> : Regularization technique prevents the model from overfitting.<br />Regularization technique (It regulates terms with higher power)<br /><br /><strong>Cause of Overfitting</strong><br /><br />    -&gt; <strong>Multicolinearity</strong> <br />    Let's understand,<br />        <br />        Attribute: Columns/Properties<br />        Features: Those columns that contributes towards model building. Features are subset of attribute.<br />        <br />    Let say we given a table and it contains 10 attributes. <br />    X= (X1, X2, X3,.... X10) <br />        <br />    Now for model building let's say we choose 4 attributes namely X1,X3,X7,X10 and so we introduced certain bias here. <br />    <br />    In order to neutralise the bias we inroduce some variance. Say we also used X6 and X8 as feature. <br />    <br />    Now if we see that X1 is linearly related to X6(i.e. info we getting from X1 is very similar to X6 so there is no use of including X6 as it just increasing the complexity) then we have introduced features that are corelated to each other and this leads to overfitting.<br />    <br /><strong>Case study</strong><br />Let's see a secenrio where we have 15 attributes<br /><br /><img src="images/4-41.png" alt="images/4-41.png" />       <img src="images/4-42.png" alt="images/4-42.png" /><br /><br />Now on selecting different number of features we get different curves<br /><br /><img src="images/4-43.png" alt="images/4-43.png" /><br /><br />Now we can see the table with values of coefficient of model in different scenerios<br /><br /><img src="images/4-44.png" alt="images/4-44.png" /><br /><br />Point to remember is that the coefficient value decides the importance of that feature(it's worth to include/considered or not).<br /><br />So to manage overfitting we have to penalise the coefficients. It cold be done as:<br /><br /> <strong>Method 1</strong>: <strong>RIDGE REGRESSION</strong> <strong>/ Ridge regularization</strong><br /><br />    -&gt; It performs L2 Regularization<br />    -&gt; The penalty added here is Sum of Squares of coefficients.<br />    <br />    -&gt; <strong>Objective</strong> : <strong>Residue Sum Square + Î±(Sum of squares of coefficients) </strong><br />    <br />    Where <strong>Î±</strong> = Hyperparameter given by the model developer. This is what the developer can change.<br />    <br />        Case 1: <strong>Î±</strong>=0<br />                     Model becomes simple linear regression<br />        <br />        Case 2: <strong>Î±</strong>=âˆž<br />                     All coefficient become 0. <br />                     If coefficient != 0, <br />                     then (coeff)^2 = <strong>âˆž</strong>, and the model will not be practically realisable.<br />                     <br />        Case 3: 0&lt;<strong>Î±</strong>&lt;<strong>âˆž</strong><br />        <br />    Let's see for different values of <strong>Î±</strong> how the model is changing.<br />    <br />    <img src="images/4-45.png" alt="images/4-45.png" /><br />                    <br />    We found that last graph with hight value of <strong>Î±</strong> is highly bias(curve is not affected by the observations).<br />    <br />    So we can conclude that <br />    -&gt; <strong>Bias is proportional to Î±</strong><br />    -&gt; With higher <strong>Î±</strong> complexity decreases<br />    <br />  <img src="images/4-46.png" alt="images/4-46.png" /><br /><br /><br /><strong>Method 2</strong>: <strong>LASSO REGRESSION</strong><br />        <br />    -&gt; Objective: <strong>Residue Sum Square + Î±(Sum of absolute value of coefficients) </strong><br />    <br />    -&gt;LASSO can make some coefficient to zero hence used while feature selection.<br /><br />    -&gt; Very sensitive as it removes features in case they don't contribute in model building by making them zero.<br />    <br />    <strong>Case Study</strong><br />    Let's say we used Lasso Regression and the table we found for different Î± values be<br />    <br />    <img src="images/4-47.png" alt="images/4-47.png" /><br />    <br /><img src="images/4-48.png" alt="images/4-48.png" />    <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Classification</strong><br />    Let's assume a supervised learning, but here the target variable(Y) is discrete/categorical.<br />    <br />    Let's see a binary classifier(only 2 classes)<br />    <br />    <img src="images/4-49.png" alt="images/4-49.png" /><br />    Now if the observation lies below the boundary then it belongs to class II else it belongs to class I.<br />    <br />    So the main objective here is to set the boundary.<br />    <br />    <strong>Note</strong>: This boundary can be a curvy line.<br />    <br />    <strong>Training a classifier means, finding the best possible threshold</strong>.<br />    <br />    <img src="images/4-50.png" alt="images/4-50.png" /><br />    <br />    So to do that we have three ways<br />    <br />   <strong> 1)</strong> <strong>Logistic Regression</strong><br />        <img src="images/4-51.png" alt="images/4-51.png" /><br />    <br />    <br />    <strong>2)</strong> <strong> Maximum Likelihood Estimator</strong><br />    <br />    <img src="images/4-52.png" alt="images/4-52.png" /><br />    <br />    <strong>3) Naive Bayes<br />     </strong><img src="images/4-53.png" alt="images/4-53.png" /><br />    <br /><br />    <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Generalised Linear Model</strong><br />    <br />    Note: When the error distribution around the points follows normal distribution then we can use linear model.<br />    <img src="images/4-54.png" alt="images/4-54.png" /><br /><br />  So we need to make a generalised linear model<br />  <br />  <img src="images/4-55.png" alt="images/4-55.png" /><br />    <br />    <img src="images/4-56.png" alt="images/4-56.png" /><br />    <br />    In poisson distribution we are given the mean(Lambda) and asked to calculate the probability.<br />    <br />    <img src="images/4-57.png" alt="images/4-57.png" /><br />    <br />    <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><strong>Maximum Likelihood</strong><br /><br />    It answers, which are the best parameters/coefficient for any model.<br />    <br />    Maximum likelihood Estimator gets the parameters(mean and variance) from the distribution.<br />    <br />    <br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><img src="images/4-58.png" alt="images/4-58.png" /><br /><br /><strong>Properties of cluster</strong><br /><br />    -&gt; All the data points in a cluster should be similar(distance between them should be least) to each other.<br />    <br />                    <img src="images/4-59.png" alt="images/4-59.png" /><br />                    <br />    -&gt; The data points from different clusters should be as different(distance between 2 points in 2 clusters should be max) as possible.<br />    <br />    <img src="images/4-60.png" alt="images/4-60.png" /><br />    <br />    <strong>Note</strong>: Case 2 is more appropriate.<br />    <br />    <br />    <strong>Understanding the different evaluation metrices for clustering</strong><br />    <br />    -&gt; Distance within a cluster- Intra cluster Distance<br />    -&gt; Distance among clusters- Inter cluster Distance<br />    <br />    1) <strong>Inertia</strong><br />        It calculates the sum of distances of all the points within a cluster from the centroid of that cluster.<br />        <img src="images/4-61.png" alt="images/4-61.png" />  <img src="images/4-62.png" alt="images/4-62.png" /><br />        <br />        If the Inertia is less then cluster is tight/compact.<br />        If the Inertia is more then cluster is loose.<br />        <br />        <img src="images/4-63.png" alt="images/4-63.png" /><br />        <br />     <strong>Higher Dunn index is better</strong><br />        <br />    <br /><strong>    K-Mean Clustering</strong><br />        Here our aim is to minimize the distance between the points within a cluster(Intracluster distance).<br />    <br />    <img src="images/4-64.png" alt="images/4-64.png" /><br />    <br />    <strong>Steps to find the K Mean cluster</strong><br />        <br />        Step <strong>1</strong>: Choose the number of cluster k<br />                     Let's say k=2<br />                        <img src="images/4-65.png" alt="images/4-65.png" /><br />                        <br />        Step <strong>2</strong>: Select k random points from the data as centroid.<br />                             <img src="images/4-66.png" alt="images/4-66.png" /><br />                             <br />        Step <strong>3</strong>: Assign all the points to the closest cluster centroid<br />                               <img src="images/4-67.png" alt="images/4-67.png" /><br />                            <br />        Step <strong>4</strong>: Recompute the centroids of newly formed clusters<br />                            <img src="images/4-68.png" alt="images/4-68.png" /><br />                            <br />        Step <strong>5</strong> : Reassign the points to the new closest cluster centroid.<br />        <br />                             <img src="images/4-69.png" alt="images/4-69.png" /><br />                             <br />  <br />    <br />    <br />    <br />    <br />    <br /><br /><br /><br /><br /></div></body></html>